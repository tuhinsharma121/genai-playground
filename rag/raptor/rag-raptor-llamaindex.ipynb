{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21267ea4-4548-44d2-b355-058dcc4fc03c",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "\n",
    "We would like to create RAPTOR powered Question-Answering system based on some publicly available arxiv research papers in finance domain using OpenAI LLM on local system.\n",
    "\n",
    "For this, we shall create a RAG application with RAPTOR retriever using OpenAI API. We shall feed the documents in it. After the RAG pipeline is ready, we shall use the questions to get responses from the RAG pipeline.\n",
    "\n",
    "So the steps are as follows:\n",
    "\n",
    "1. Download 2 research papers from arxiv.\n",
    "2. Ingest the documents into a chroma vectordb through RAPTOR method\n",
    "3. Define the Retriever\n",
    "4. Define the RAG pipeline\n",
    "5. Ask questions to the RAG application.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4e2ad2-6970-4586-ace3-18b9b3348036",
   "metadata": {},
   "source": [
    "# Initialize modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "927dcb6c-b686-4865-83bb-4b5d086a9285",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "import os\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = \"your-openai-api-key\"\n",
    "    \n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938d50cf-e196-46f6-8e98-18db9083a9e9",
   "metadata": {},
   "source": [
    "# 1. Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01a22402-cb58-40bc-96a3-97666511047e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-03-10 17:06:55--  https://arxiv.org/pdf/2309.13064\n",
      "Resolving arxiv.org (arxiv.org)... 151.101.67.42, 151.101.3.42, 151.101.131.42, ...\n",
      "Connecting to arxiv.org (arxiv.org)|151.101.67.42|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 230648 (225K) [application/pdf]\n",
      "Saving to: ‘./invest_lm.pdf’\n",
      "\n",
      "./invest_lm.pdf     100%[===================>] 225.24K  --.-KB/s    in 0.04s   \n",
      "\n",
      "2025-03-10 17:06:55 (4.95 MB/s) - ‘./invest_lm.pdf’ saved [230648/230648]\n",
      "\n",
      "--2025-03-10 17:06:55--  https://arxiv.org/pdf/2306.12659\n",
      "Resolving arxiv.org (arxiv.org)... 151.101.3.42, 151.101.131.42, 151.101.195.42, ...\n",
      "connected. to arxiv.org (arxiv.org)|151.101.3.42|:443... \n",
      "200 OKequest sent, awaiting response... \n",
      "Length: 247127 (241K) [application/pdf]\n",
      "Saving to: ‘./instruct_fingpt.pdf’\n",
      "\n",
      "./instruct_fingpt.p 100%[===================>] 241.33K  --.-KB/s    in 0.04s   \n",
      "\n",
      "2025-03-10 17:06:55 (5.40 MB/s) - ‘./instruct_fingpt.pdf’ saved [247127/247127]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://arxiv.org/pdf/2309.13064 -O ./invest_lm.pdf\n",
    "!wget https://arxiv.org/pdf/2306.12659 -O ./instruct_fingpt.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86ab3fb-0017-47ae-ba50-f5560530a4e8",
   "metadata": {},
   "source": [
    "# 2. Ingest Finance research papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95d7a5b-9f92-494a-8170-92200b9d09bb",
   "metadata": {},
   "source": [
    "## 2.1 Load documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "917b0b82-f5d8-4bc1-a533-aac9223c2ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "fin_documents = SimpleDirectoryReader(input_files=[\"./invest_lm.pdf\",\"./instruct_fingpt.pdf\"]).load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb23f47-b3a3-4e05-a425-38b1bda40ae1",
   "metadata": {},
   "source": [
    "## 2.2 Instantiate vectordb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fac8536-fdc8-4475-948e-d277a40b8e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "\n",
    "client = chromadb.PersistentClient(path=\"./finance_knowledge_db\")\n",
    "collection = client.get_or_create_collection(\"fin_raptor\")\n",
    "\n",
    "vector_store = ChromaVectorStore(chroma_collection=collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1f0b0c-34ad-428e-9261-f029dcecb188",
   "metadata": {},
   "source": [
    "## 2.3 Define the summary module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3206f03-b752-49e5-810d-3a013b7fb2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.packs.raptor.base import SummaryModule\n",
    "\n",
    "summary_prompt = \"As a professional summarizer, create a concise and comprehensive summary of the provided text, \\\n",
    "                    be it an article, post, conversation, or passage with as much detail as possible.\"\n",
    "\n",
    "summary_module = SummaryModule(\n",
    "    llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1), summary_prompt=summary_prompt, num_workers=16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3b4762-ba5b-4e03-b733-b858363ef1c5",
   "metadata": {},
   "source": [
    "## 2.4 Define the RAPTOR PACK and ingest the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f60f65e4-111b-4743-b1e6-4721b4f95817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for level 0.\n",
      "Performing clustering for level 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/raptor-dev/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating summaries for level 0 with 10 clusters.\n",
      "Level 0 created summaries/clusters: 10\n",
      "Generating embeddings for level 1.\n",
      "Performing clustering for level 1.\n",
      "Generating summaries for level 1 with 1 clusters.\n",
      "Level 1 created summaries/clusters: 1\n",
      "Generating embeddings for level 2.\n",
      "Performing clustering for level 2.\n",
      "Generating summaries for level 2 with 1 clusters.\n",
      "Level 2 created summaries/clusters: 1\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.packs.raptor import RaptorPack\n",
    "\n",
    "\n",
    "raptor_pack = RaptorPack(\n",
    "    fin_documents,\n",
    "    embed_model=OpenAIEmbedding(\n",
    "        model=\"text-embedding-3-small\"\n",
    "    ),  # used for embedding clusters\n",
    "    vector_store=vector_store,  # used for storage\n",
    "    similarity_top_k=2,  # top k for each layer, or overall top-k for collapsed\n",
    "    mode=\"collapsed\",  # sets default mode\n",
    "    transformations=[\n",
    "        SentenceSplitter(chunk_size=400, chunk_overlap=50)\n",
    "    ],  # transformations applied for ingestion\n",
    "    summary_module=summary_module,  # used for generating summaries\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c872471e-a45e-4559-a1b4-9c63237a2f70",
   "metadata": {},
   "source": [
    "## 2.5 Test retrieval using collapsed mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5c2a39c-8ecc-4af7-bf0e-5bd07901af6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Baselines. We compare InvestLM with three state-\n",
      "of-the-art commercial models, GPT-3.5, GPT-4\n",
      "and Claude-2. OpenAI’s GPT-3.5 and GPT-4 are\n",
      "large language models tuned with reinforcement\n",
      "learning from human feedback (RLHF) (Ouyang\n",
      "et al., 2022). Anthropic’s Claude-2 is a large lan-\n",
      "guage model that can take up to 100K tokens in the\n",
      "user’s prompt. 3 Responses from all baselines are\n",
      "sampled throughout August 2023.\n",
      "We manually write 30 test questions that are\n",
      "related to financial markets and investment. For\n",
      "each question, we generate a single response from\n",
      "InvestLM and the three commercial models. We\n",
      "then ask the financial experts to compare InvestLM\n",
      "responses to each of the baselines and label which\n",
      "response is better or whether neither response is\n",
      "significantly better than the other.\n",
      "In addition to the expert evaluation, we also con-\n",
      "duct a GPT-4 evaluation, following the same pro-\n",
      "tocol used in (Zhou et al., 2023). Specifically, we\n",
      "send GPT-4 with exactly the same instructions and\n",
      "data annotations, and ask GPT-4 which response is\n",
      "better or whether neither response is significantly\n",
      "better than the other. The expert evaluation inter-\n",
      "face and GPT-4 evaluation prompt are presented in\n",
      "Appendix B.\n",
      "The expert evaluation and GPT-4 evaluation\n",
      "results are presented in Figure 1 and Figure 2.\n",
      "These results indicate that financial experts rate\n",
      "InvestLM’s responses as either comparable to or\n",
      "better than those of the GPT-3.5 and GPT-4 mod-\n",
      "els.\n"
     ]
    }
   ],
   "source": [
    "nodes = raptor_pack.run(\"What baselines is InvestLM compared against?\", mode=\"collapsed\")\n",
    "print(len(nodes))\n",
    "print(nodes[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb4a847-1907-460a-b998-97407749bc1d",
   "metadata": {},
   "source": [
    "## 2.6 Test retrieval using tree_traversal mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6658f553-b28d-4840-b525-03c95a55a518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved parent IDs from level 2: ['bd04e6bd-2b35-4cd7-a0c0-805a7d7d96cd']\n",
      "Retrieved 1 from parents at level 2.\n",
      "Retrieved parent IDs from level 1: ['0900c503-731b-4da1-9ad9-ec74cc600235']\n",
      "Retrieved 2 from parents at level 1.\n",
      "Retrieved parent IDs from level 0: ['881c7015-ad05-4454-9b0b-b52412c5b08f', '973bf4b8-70ce-4230-bb5a-7ae84b83b98f']\n",
      "Retrieved 4 from parents at level 0.\n",
      "4\n",
      "Baselines. We compare InvestLM with three state-\n",
      "of-the-art commercial models, GPT-3.5, GPT-4\n",
      "and Claude-2. OpenAI’s GPT-3.5 and GPT-4 are\n",
      "large language models tuned with reinforcement\n",
      "learning from human feedback (RLHF) (Ouyang\n",
      "et al., 2022). Anthropic’s Claude-2 is a large lan-\n",
      "guage model that can take up to 100K tokens in the\n",
      "user’s prompt. 3 Responses from all baselines are\n",
      "sampled throughout August 2023.\n",
      "We manually write 30 test questions that are\n",
      "related to financial markets and investment. For\n",
      "each question, we generate a single response from\n",
      "InvestLM and the three commercial models. We\n",
      "then ask the financial experts to compare InvestLM\n",
      "responses to each of the baselines and label which\n",
      "response is better or whether neither response is\n",
      "significantly better than the other.\n",
      "In addition to the expert evaluation, we also con-\n",
      "duct a GPT-4 evaluation, following the same pro-\n",
      "tocol used in (Zhou et al., 2023). Specifically, we\n",
      "send GPT-4 with exactly the same instructions and\n",
      "data annotations, and ask GPT-4 which response is\n",
      "better or whether neither response is significantly\n",
      "better than the other. The expert evaluation inter-\n",
      "face and GPT-4 evaluation prompt are presented in\n",
      "Appendix B.\n",
      "The expert evaluation and GPT-4 evaluation\n",
      "results are presented in Figure 1 and Figure 2.\n",
      "These results indicate that financial experts rate\n",
      "InvestLM’s responses as either comparable to or\n",
      "better than those of the GPT-3.5 and GPT-4 mod-\n",
      "els.\n"
     ]
    }
   ],
   "source": [
    "nodes = raptor_pack.run(\n",
    "    \"What baselines is InvestLM compared against?\", mode=\"tree_traversal\"\n",
    ")\n",
    "print(len(nodes))\n",
    "print(nodes[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf83911-3fc4-4009-a28d-9c65669b67df",
   "metadata": {},
   "source": [
    "# 3. Define the Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f60a6517-0b11-46d8-9d25-28fe74b687ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.packs.raptor import RaptorRetriever\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.llms.openai import OpenAI\n",
    "import chromadb\n",
    "\n",
    "client = chromadb.PersistentClient(path=\"./finance_knowledge_db\")\n",
    "collection = client.get_or_create_collection(\"fin_raptor\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=collection)\n",
    "\n",
    "retriever = RaptorRetriever(\n",
    "    [],\n",
    "    embed_model=OpenAIEmbedding(\n",
    "        model=\"text-embedding-3-small\"\n",
    "    ),  \n",
    "    vector_store=vector_store,  # used for storage\n",
    "    similarity_top_k=2,  # top k for each layer, or overall top-k for collapsed\n",
    "    mode=\"tree_traversal\",  # sets default mode\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7197c39-5866-4e17-9055-29eb23ff6eb7",
   "metadata": {},
   "source": [
    "# 4. Define the Query Engine (RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee28d5f7-d2ed-4f21-82a3-930e5e70e912",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "query_engine = RetrieverQueryEngine.from_args(\n",
    "    retriever, llm=OpenAI(model=\"gpt-4o-mini\", temperature=0.1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0474794-fec4-4cf6-99f5-3d7ed23bb106",
   "metadata": {},
   "source": [
    "# 5. Test the query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec7bff17-5e97-49ba-bbf3-be5b9a5cd1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InvestLM is compared against three state-of-the-art commercial models: GPT-3.5, GPT-4, and Claude-2.\n"
     ]
    }
   ],
   "source": [
    "query = \"What baselines is InvestLM compared against?\"\n",
    "response = query_engine.query(query)\n",
    "print(str(response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
