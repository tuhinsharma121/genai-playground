{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acda52e9-3fda-4663-9047-7a78f06c166b",
   "metadata": {},
   "source": [
    "\n",
    "We would like to create Question-Answering system for my medium articles on GenAI using OpenAI LLM on local system.\n",
    "\n",
    "For this, we shall create an adaptive RAG application using OpenAI API. We shall feed the documents in it. After the RAG pipeline is ready, we shall use the questions to get responses from the RAG pipeline and compare them with the ground truth answers.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc998352-719b-4c7e-a961-276707ad6223",
   "metadata": {},
   "source": [
    "![title](adaptive-rag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d0619c-84e6-4a3f-a518-c8713b16a77d",
   "metadata": {},
   "source": [
    "So the steps are as follows:\n",
    "\n",
    "1. [Retrieve Node] Ingest Tuhin's articles in ChromaDB2. Define the Router (when to get documents from ChromaDB and when to get it from web)\n",
    "2. [Decision - Route Query] Define the Query analysis logic\n",
    "3. [Grader Node] Define Retrieval Grader\n",
    "4. [Generate Node] Write Generation logic\n",
    "5. [Decision] - Hallucination and Answer Grader\n",
    "6. [Rewrite Question Node]\n",
    "7. [Web Search Node]\n",
    "8. [Decision - Generate or Rewrite question]\n",
    "9. Construct the Agentic Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e691bf7-09ab-438e-b15f-119691f6936f",
   "metadata": {},
   "source": [
    "# 1. [Retrieve Node] Ingest Tuhin's articles in ChromaDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563c8a8d-16f5-4bed-98b1-3940f9fd0a46",
   "metadata": {},
   "source": [
    "The following code blocks does the following\n",
    "- Loads a set of blog articles from the web.\n",
    "- Splits them into manageable text chunks.\n",
    "- Converts each chunk into an embedding vector.\n",
    "- Stores them in a vector DB (Chroma).\n",
    "- Sets up a retriever to query these chunks later for RAG-based applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4885d386-a42c-4941-bee2-2cb6c51d65ec",
   "metadata": {},
   "source": [
    "## 1.1. Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a951a16-921e-4f84-8db9-542a054ee8d3",
   "metadata": {},
   "source": [
    "- WebBaseLoader: Used to fetch content from web pages.\n",
    "- RecursiveCharacterTextSplitter: Splits large texts into smaller chunks while keeping logical structure.\n",
    "- Chroma: A vector database to store and retrieve documents using embeddings.\n",
    "- OpenAIEmbeddings: Generates vector representations of text using OpenAI’s embedding models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e60ad9d-874f-4782-ab7a-02ab666835ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b036e204-b561-415f-9d3b-b70dec546284",
   "metadata": {},
   "source": [
    "## 1.2. Set Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ce66c6-da23-4ee5-81b2-7773de5c6199",
   "metadata": {},
   "source": [
    "Sets up OpenAI embeddings (like text-embedding-ada-002) to convert text chunks into vectors for similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f912909c-0788-4303-8042-648bf1bddb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embd = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5959f23-ccd5-4634-959d-2749362846d1",
   "metadata": {},
   "source": [
    "## 1.3. Specify Web URLs to Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9283201-46e4-40f8-a334-d3a9ad61c2a4",
   "metadata": {},
   "source": [
    "These are links to blog posts by me, likely technical articles on AI and transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c453220-c1c3-4fcb-a15b-316a40438e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://medium.com/@tuhinsharma121/raptor-a-smarter-way-to-retrieve-and-use-information-in-ai-fd3cb68a6f2f\",\n",
    "    \"https://medium.com/@tuhinsharma121/building-a-biomedical-question-answering-system-using-rag-and-openai-llm-b9c3502fd287\",\n",
    "    \"https://medium.com/@tuhinsharma121/llamaindex-vs-langchain-vs-haystack-vs-llama-stack-a-comparative-analysis-6d03aaa1bc36\",\n",
    "    \"https://medium.com/@tuhinsharma121/riding-multi-headed-dragons-decoding-the-power-of-multi-head-attention-in-transformers-7c9d18dc2b68\",\n",
    "    \"https://medium.com/@tuhinsharma121/decoding-the-magic-of-transformers-a-deep-dive-into-input-embeddings-and-positional-encoding-a9a282f4e055\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97fe272-acec-4ad8-a289-ded3869295b0",
   "metadata": {},
   "source": [
    "## 1.4. Load Web Content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da457fc-5a40-43fb-8bb7-378d5e8cfb2c",
   "metadata": {},
   "source": [
    "- For each URL, it uses WebBaseLoader to download and parse the content into Document objects.\n",
    "- docs becomes a list of lists (one per URL), so it’s flattened into docs_list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b13ad24-230f-4e2d-a7c7-dab35af172cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a5e263-8d0f-4201-a0ab-345de605cd8b",
   "metadata": {},
   "source": [
    "## 1.5. Split Text into Chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b08a8d-9b09-47ba-9397-dac39022623f",
   "metadata": {},
   "source": [
    "- Splits each document into 500-character chunks (no overlap).\n",
    "- This helps avoid context limits in LLMs and makes retrieval more granular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41e53dfb-940b-4fa1-bd72-35493884d3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=500, chunk_overlap=0\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d1bdde-4de7-42fa-b013-25ed2fff224b",
   "metadata": {},
   "source": [
    "## 1.6. Create Vector Store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2a4a16-e7a5-4f8b-a51e-702ca3642e13",
   "metadata": {},
   "source": [
    "- Converts each chunk into a vector using OpenAI embeddings.\n",
    "- Stores those vectors in a Chroma DB collection named \"tuhin-blogs\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33fe0f8d-90a2-4819-b3fd-838c98a0815b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"tuhin-blogs\",\n",
    "    embedding=embd,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0099a5b7-db0f-4b99-bcce-659a3e7cefec",
   "metadata": {},
   "source": [
    "## 1.7. Create Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e74e642-3e72-44da-a7be-b14cbb345839",
   "metadata": {},
   "source": [
    "Creates a retriever object that can now be used to search for relevant chunks using similarity-based lookup during question answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73ca44eb-00bb-46fe-8117-b13a7116e24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a180974f-ebac-4a77-82f3-04bcd5cc4da4",
   "metadata": {},
   "source": [
    "## 1.8 Create Retriever function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8181a645-e83f-44dc-a299-f871208d42b2",
   "metadata": {},
   "source": [
    "This function retrieve(state) is designed to retrieve documents based on a question from the state dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22f757bb-3b29-467c-b34e-dd44c9323e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RETRIEVE---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'documents': [Document(metadata={'description': 'Large language models (LLMs) like ChatGPT and GPT-4 are incredibly powerful, but they struggle to keep up with new information and understand long, complex documents. Traditional retrieval methods…', 'language': 'en', 'source': 'https://medium.com/@tuhinsharma121/raptor-a-smarter-way-to-retrieve-and-use-information-in-ai-fd3cb68a6f2f', 'title': 'RAPTOR: A Smarter Way to Retrieve and Use Information in AI | by Tuhin Sharma | Mar, 2025 | Medium'}, page_content='RAPTOR: A Smarter Way to Retrieve and Use Information in AI | by Tuhin Sharma | Mar, 2025 | MediumOpen in appSign upSign inWriteSign upSign inHomeFollowingLibraryYour listsSaved listsHighlightsReading historyStoriesStatsRAPTOR: A Smarter Way to Retrieve and Use Information in AITuhin Sharma·Follow8 min read·Mar 10, 2025--ListenShareGenerated on MidjourneyIntroductionLarge language models (LLMs) like ChatGPT and GPT-4 are incredibly powerful, but they struggle to keep up with new information and understand long, complex documents. Traditional retrieval methods pull short chunks of text, often missing important context. This is where RAPTOR (Recursive Abstractive Processing for Tree-Organized Retrieval) comes in — a new way to retrieve and summarize information efficiently. In this article, we’ll break down what RAPTOR is, why it’s better than traditional methods, and how to implement it with a hands-on example.The Problem with Traditional RetrievalMost retrieval-augmented models work by breaking down documents into small chunks and retrieving only the most relevant ones. However, this approach has some limitations:Loss of Context: Retrieving only small, isolated chunks may miss the bigger picture especially for documents with long contexts.Difficulty in Multi-Step Reasoning: Some questions require information from multiple sections of a document.High Computational Cost: Processing large amounts of text in small pieces increases memory usage and slows down the system.What is RAPTOR?RAPTOR is a new retrieval system that organizes information in a tree structure. Instead of retrieving small, isolated chunks, RAPTOR recursively clusters and summarizes text, creating a hierarchy of information. This allows it to retrieve both fine-grained details and high-level summaries based on the question asked.Credit : Parth Sarthi et alHow Does RAPTOR Work?Chunking and Embedding: RAPTOR first breaks the document into small text chunks and converts them into vector embeddings.Clustering and Summarization: It groups similar chunks together and generates summaries.Building a Tree Structure: The process repeats at multiple levels, forming a hierarchical tree.Retrieval and Querying: When a user asks a question, RAPTOR intelligently pulls information from different levels of the tree to'),\n",
       "  Document(metadata={'description': 'Large language models (LLMs) like ChatGPT and GPT-4 are incredibly powerful, but they struggle to keep up with new information and understand long, complex documents. Traditional retrieval methods…', 'language': 'en', 'source': 'https://medium.com/@tuhinsharma121/raptor-a-smarter-way-to-retrieve-and-use-information-in-ai-fd3cb68a6f2f', 'title': 'RAPTOR: A Smarter Way to Retrieve and Use Information in AI | by Tuhin Sharma | Mar, 2025 | Medium'}, page_content='provide a well-rounded answer.Why is RAPTOR Better?Improved Comprehension: It captures both fine details and big-picture themes.Efficient Retrieval: It retrieves relevant summaries, reducing computation time.Better Performance: RAPTOR achieves state-of-the-art results on multiple question-answering benchmarks.Example: Building a RAPTOR-powered QA SystemLet’s walk through an example where we use RAPTOR to build a Question-Answering (QA) system for finance research papers using OpenAI’s API. The code can be found here on github. If you like the code do not forget to star the repo.Tools Used:ChromaDB: A vector database for storing document embeddings.LlamaIndex: A tool for building and querying AI-powered knowledge bases.OpenAI API: Provides GPT-4 for generating summaries and answering questions.Step 1: Install Required LibrariesClone the repo and move to the folder containing the notebook!pip install -r requirements.txtStep 2: Set Up API KeySet your OpenAI api keyimport nest_asyncioimport osif \"OPENAI_API_KEY\" not in os.environ:    os.environ[\"OPENAI_API_KEY\"] = \"your-openai-api-key\"    nest_asyncio.apply()Step 3: Download Sample Research PapersWe download 2 research papers on LLMs in the finance domain. The goal is to get these 2 document ingested and use them to show the power of RAPTOR.!wget https://arxiv.org/pdf/2309.13064 -O ./invest_lm.pdf!wget https://arxiv.org/pdf/2306.12659 -O ./instruct_fingpt.pdf--2025-03-10 17:06:55--  https://arxiv.org/pdf/2309.13064Resolving arxiv.org (arxiv.org)... 151.101.67.42, 151.101.3.42, 151.101.131.42, ...Connecting to arxiv.org (arxiv.org)|151.101.67.42|:443... connected.HTTP request sent, awaiting response... 200'),\n",
       "  Document(metadata={'description': 'Large language models (LLMs) like ChatGPT and GPT-4 are incredibly powerful, but they struggle to keep up with new information and understand long, complex documents. Traditional retrieval methods…', 'language': 'en', 'source': 'https://medium.com/@tuhinsharma121/raptor-a-smarter-way-to-retrieve-and-use-information-in-ai-fd3cb68a6f2f', 'title': 'RAPTOR: A Smarter Way to Retrieve and Use Information in AI | by Tuhin Sharma | Mar, 2025 | Medium'}, page_content='retriever, llm=OpenAI(model=\"gpt-4o-mini\", temperature=0.1))Step 10: Ask a QuestionNow that the setup is ready lets ask the same question again.query = \"What baselines is InvestLM compared against?\"response = query_engine.query(query)print(str(response))This will return an answer that is both accurate and contextually relevant, thanks to RAPTOR’s structured retrieval system as follows.InvestLM is compared against three state-of-the-art commercial models: GPT-3.5, GPT-4, and Claude-2.Real-World ImpactRAPTOR can be used for tasks like:Legal Document Analysis: Understanding long contracts and case laws.Scientific Research Summaries: Extracting key findings from lengthy papers.Financial Reports: Analyzing large datasets for trends and insights.ConclusionRAPTOR is a game-changer for retrieval-augmented AI applications. By using recursive clustering and summarization, it allows for more efficient and context-aware retrieval of information. Whether for finance, legal, or academic research, RAPTOR provides a smarter way to handle large-scale documents. If you’re looking to build a high-performance AI system that needs to retrieve and process long documents, RAPTOR is the way to go!Want to try it out? Implement the example above and explore how RAPTOR enhances your AI applications!References[1] https://arxiv.org/html/2401.18059v1[2] https://docs.llamaindex.ai/en/stable/api_reference/packs/raptor/[3] https://github.com/tuhinsharma121/genai-playgroundRaptorLlamaindexGenaiRetrieval Augmented Gen----FollowWritten by Tuhin Sharma32 Followers·9 FollowingSenior Principal Data Scientist, Data & AI @ Red HatFollowNo responses yetHelpStatusAboutCareersPressBlogPrivacyRulesTermsText to speech'),\n",
       "  Document(metadata={'description': 'Large language models (LLMs) like ChatGPT and GPT-4 are incredibly powerful, but they struggle to keep up with new information and understand long, complex documents. Traditional retrieval methods…', 'language': 'en', 'source': 'https://medium.com/@tuhinsharma121/raptor-a-smarter-way-to-retrieve-and-use-information-in-ai-fd3cb68a6f2f', 'title': 'RAPTOR: A Smarter Way to Retrieve and Use Information in AI | by Tuhin Sharma | Mar, 2025 | Medium'}, page_content='summaries for level 1 with 1 clusters.Level 1 created summaries/clusters: 1Generating embeddings for level 2.Performing clustering for level 2.Generating summaries for level 2 with 1 clusters.Level 2 created summaries/clusters: 1Step 8: Query RAPTOR for AnswersThis retrieves the most relevant answer from the structured data. This step is heavily influenced by how we want to retrieve the documents from the vector store. There are basically two options.Credit : Parth Sarthi et alCollapsed tree collapses the tree into a single layer and retrieves nodes until a threshold number of tokens is reached, based on cosine similarity to the query vector. The nodes on which cosine similarity search is performed are highlighted in both illustrations.nodes = raptor_pack.run(    \"What baselines is InvestLM compared against?\", mode=\"collapsed\")print(len(nodes))print(nodes[0].text)2Baselines. We compare InvestLM with three state-of-the-art commercial models, GPT-3.5, GPT-4and Claude-2. OpenAI’s GPT-3.5 and GPT-4 arelarge language models tuned with reinforcementlearning from human feedback (RLHF) (Ouyanget al., 2022). Anthropic’s Claude-2 is a large lan-guage model that can take up to 100K tokens in theuser’s prompt. 3 Responses from all baselines aresampled throughout August 2023.We manually write 30 test questions that arerelated to financial markets and investment. Foreach question, we generate a single response fromInvestLM and the three commercial models. Wethen ask the financial experts to compare InvestLMresponses to each of the baselines and label whichresponse is better or whether neither response issignificantly better than the other.In addition to the expert evaluation, we also con-duct a GPT-4 evaluation, following the same pro-tocol used in (Zhou et al., 2023). Specifically, wesend GPT-4 with exactly the same instructions anddata annotations, and ask GPT-4 which response isbetter or whether neither response is significantlybetter than the other. The expert evaluation inter-face and GPT-4 evaluation prompt are presented inAppendix B.The expert evaluation and GPT-4')],\n",
       " 'question': 'What is RAPTOR?'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Retrieval\n",
    "    documents = retriever.invoke(question)\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "state = {\"question\": \"What is RAPTOR?\"}\n",
    "retrieve(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0bdf44-5703-4e7b-9d0a-6f8584e060fa",
   "metadata": {},
   "source": [
    "# 2. [Decision - Route Query] Define the Query analysis logic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c5951f-5d8e-452b-82b0-e91b8b374d5f",
   "metadata": {},
   "source": [
    "This code is an example of LLM-powered decision routing using structured output and LangChain. It:\n",
    "- Defines two routes: vectorstore and web_search\n",
    "- Instructs the LLM to choose the right one based on topic\n",
    "- Leverages OpenAI’s function-calling to enforce a fixed schema\n",
    "- Demonstrates how to use prompts to implement logic without writing if-else code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67e9d07-2477-4fac-af55-6cf67a1db3f2",
   "metadata": {},
   "source": [
    "## 2.1. Imports Required Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baaa389b-0872-4d6f-b7f2-67d91d3c5365",
   "metadata": {},
   "source": [
    "Brings in typing support (Literal), data modeling (BaseModel), and LangChain components for prompt engineering and LLM interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a308ce0-6503-43a1-8bdd-93d5981ed7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b0ce4e-8e69-409f-95e4-65869c963a02",
   "metadata": {},
   "source": [
    "## 2.2. Define Output Schema with Pydantic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e336a106-5e54-48e9-b684-4514eee9bc0c",
   "metadata": {},
   "source": [
    "The data model `RouteQuery` returns `datasource='web_search'` or `datasource='vectorstore'` based on the input question. Note the `system_prompt` includes the logic of the routing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "897a5d62-e9c3-4f7b-a5bc-d18106569235",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n",
    "\n",
    "    datasource: Literal[\"vectorstore\", \"web_search\"] = Field(\n",
    "        ...,\n",
    "        description=\"Given a user question choose to route it to web search or a vectorstore.\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0d1824-976a-423f-bd6d-57b6e97f712f",
   "metadata": {},
   "source": [
    "## 2.3. Set Up the LLM with Structured Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f85586b-f499-4290-b9a5-81e1d03589a8",
   "metadata": {},
   "source": [
    "Uses OpenAI’s GPT-3.5-turbo-0125 model with deterministic output (temperature=0).  \n",
    "Configures the LLM to follow the structure defined in RouteQuery using the function-calling paradigm, a newer method of constrained output generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07d92769-83d2-4198-9ea3-824ee318b57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "structured_llm_router = llm.with_structured_output(RouteQuery,method=\"function_calling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be79e657-1d70-452e-9ea2-00d14d7e24bc",
   "metadata": {},
   "source": [
    "## 2.4. Define a System Prompt for Routing Logic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b38b88-c7db-42ce-a35a-8d32e832cd17",
   "metadata": {},
   "source": [
    "This is the instructions for the LLM: use vectorstore for specific technical topics and web_search for anything else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "891ab6da-75f4-4606-a918-215c4bd5ebdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are an expert at routing a user question to a vectorstore or web search.\n",
    "The vectorstore contains documents related to RAPTOR, llamastack,llamaindex, langchain,haystack and transformer architecture.\n",
    "Use the vectorstore for questions on these topics. Otherwise, use web-search.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609207dc-8a48-4586-80e9-8e3d2c94e2eb",
   "metadata": {},
   "source": [
    "## 2.5. Create a Chat Prompt Template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c348cccb-3cd8-461a-a3f5-893927643cf5",
   "metadata": {},
   "source": [
    "Defines a 2-turn conversation structure: system gives instructions, human asks a question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a1e123d-c4fd-401e-8a47-2a20abd2e893",
   "metadata": {},
   "outputs": [],
   "source": [
    "route_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fd8ef1-3c2a-4f52-ab7c-3bf345f6fa15",
   "metadata": {},
   "source": [
    "## 2.6. Create a Router Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c71ac7-0061-4b8e-bc9e-c7c8195a0a64",
   "metadata": {},
   "source": [
    "Chains the prompt and the structured output LLM together into a single callable object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "248e5ef1-8aac-4b4d-bbc2-1dddc87fe142",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_router = route_prompt | structured_llm_router"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1354047c-9666-42f0-8207-2892930d96db",
   "metadata": {},
   "source": [
    "## 2.7. Test the Router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b293f896-56a0-4459-80b7-5af1a2ff0eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasource='web_search'\n",
      "datasource='vectorstore'\n"
     ]
    }
   ],
   "source": [
    "print(question_router.invoke({\"question\": \"Who will win IPL 2025?\"}))\n",
    "print(question_router.invoke({\"question\": \"What are the various components of transformer architecture?\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d886c0e7-04b4-490d-a078-930e41e1a6a2",
   "metadata": {},
   "source": [
    "- First question: “Who will win IPL 2025?”  \n",
    "    - Not related to technical topics → output: {\"datasource\": \"web_search\"}  \n",
    "- Second question: “What are the various components of transformer architecture?”  \n",
    "    - Matches the vectorstore topic list → output: {\"datasource\": \"vectorstore\"}  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896c0f86-09d6-43ec-8f42-4f92fc118cb4",
   "metadata": {},
   "source": [
    "## 2.8. Create Router Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2051bdd-b78f-4dce-8494-9f2451a0b5ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---ROUTE QUESTION---\n",
      "---ROUTE QUESTION TO RAG---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'vectorstore'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def route_question(state):\n",
    "    \"\"\"\n",
    "    Route question to web search or RAG.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ROUTE QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    source = question_router.invoke({\"question\": question})\n",
    "    if source.datasource == \"web_search\":\n",
    "        print(\"---ROUTE QUESTION TO WEB SEARCH---\")\n",
    "        return \"web_search\"\n",
    "    elif source.datasource == \"vectorstore\":\n",
    "        print(\"---ROUTE QUESTION TO RAG---\")\n",
    "        return \"vectorstore\"\n",
    "        \n",
    "state = {\"question\": \"What is RAPTOR?\"}\n",
    "route_question(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31021a3f-1646-42c1-aaf3-97336461b53b",
   "metadata": {},
   "source": [
    "# 3. [Grader Node] Define Retrieval Grader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ad40d2-3aec-4b36-a863-7ecae5a15c8b",
   "metadata": {},
   "source": [
    "## 3.1. Define Output Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d9d02b-8f1f-499f-942d-c1665b5f434d",
   "metadata": {},
   "source": [
    "This is a Pydantic data model used to constrain and structure the LLM’s output.  \n",
    "It expects a single field binary_score with the value 'yes' or 'no'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2076d53f-87c7-43fd-91ff-f2d7b171421e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97755119-4dbc-4396-8c15-1b8f87836a07",
   "metadata": {},
   "source": [
    "## 3.2. Set Up the LLM with Function Calling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2871362-eff6-4c1c-bfbe-1a94ee9343a1",
   "metadata": {},
   "source": [
    "Initializes the OpenAI chat model (gpt-3.5-turbo-0125) with temperature=0 for deterministic outputs.  \n",
    "Wraps it using with_structured_output, enabling function calling to return results in the format defined by GradeDocuments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6c92d58-fbd9-4576-91b6-8ed4f40d038f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments,method=\"function_calling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0703d557-a47f-4ba5-8864-cd74135cef1e",
   "metadata": {},
   "source": [
    "## 3.3. Create a Grading Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8489b4a7-c507-4004-99a3-7ce182970332",
   "metadata": {},
   "source": [
    "This system message instructs the LLM to act as a binary grader:\n",
    "- Mark a document 'yes' if it’s loosely relevant to the user question.\n",
    "- Mark 'no' if it’s not relevant.\n",
    "- Emphasis is on filtering out erroneous retrievals rather than perfect matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "734f0315-acea-43d4-968f-79de0f1f0a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "    If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02528b4c-d5bc-45ac-8cf9-bef7c7ad6352",
   "metadata": {},
   "source": [
    "Then, the prompt template is created using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8f452b5a-e219-492a-8abe-2cfd2192aae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd3a21a-4c4a-4755-a5ea-413abd991a7e",
   "metadata": {},
   "source": [
    "This combines:\n",
    "- the system message, and\n",
    "- a user message that plugs in a document and a question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c8ab201a-df80-4ffb-b420-b8cd01164332",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_grader = grade_prompt | structured_llm_grader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0352398d-c435-4d06-88df-fd3c99dc5b51",
   "metadata": {},
   "source": [
    "## 3.5. Run the Grader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132ec107-aad6-4548-b8b6-ac1a3156c547",
   "metadata": {},
   "source": [
    "A user question is defined: \"agent memory\".  \n",
    "A document is retrieved using retriever.invoke(question).  \n",
    "The second document (docs[1]) is selected and its content is extracted.  \n",
    "The grader pipeline is invoked with the question + document.  \n",
    "It prints whether the document is relevant (‘yes’) or not (‘no’).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d5ca7cd4-9e8a-4875-87b9-4fb90653d527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_score='no'\n"
     ]
    }
   ],
   "source": [
    "question = \"agent memory\"\n",
    "docs = retriever.invoke(question)\n",
    "doc_txt = docs[1].page_content\n",
    "print(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a37feb-2e82-43f7-8b0f-d22f3bb2cca6",
   "metadata": {},
   "source": [
    "## 3.6. Create Grader Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "306df6ce-5262-401a-8016-566b7daaace4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT RELEVANT---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'documents': [Document(metadata={'description': 'Large language models (LLMs) like ChatGPT and GPT-4 are incredibly powerful, but they struggle to keep up with new information and understand long, complex documents. Traditional retrieval methods…', 'language': 'en', 'source': 'https://medium.com/@tuhinsharma121/raptor-a-smarter-way-to-retrieve-and-use-information-in-ai-fd3cb68a6f2f', 'title': 'RAPTOR: A Smarter Way to Retrieve and Use Information in AI | by Tuhin Sharma | Mar, 2025 | Medium'}, page_content='1 created summaries/clusters: 1Generating embeddings for level 2.Performing clustering for level 2.Generating summaries for level 2 with 1 clusters.Level 2 created summaries/clusters: 1Step 8: Query RAPTOR for AnswersThis retrieves the most relevant answer from the structured data. This step is heavily influenced by how we want to retrieve the documents from the vector store. There are basically two options.Credit : Parth Sarthi et alCollapsed tree collapses the tree into a single layer and retrieves nodes until a threshold number of tokens is reached, based on cosine similarity to the query vector. The nodes on which cosine similarity search is performed are highlighted in both illustrations.nodes = raptor_pack.run(    \"What baselines is InvestLM compared against?\", mode=\"collapsed\")print(len(nodes))print(nodes[0].text)2Baselines. We compare InvestLM with three state-of-the-art commercial models, GPT-3.5, GPT-4and Claude-2. OpenAI’s GPT-3.5 and GPT-4 arelarge language models tuned with reinforcementlearning from human feedback (RLHF) (Ouyanget al., 2022). Anthropic’s Claude-2 is a large lan-guage model that can take up to 100K tokens in theuser’s prompt. 3 Responses from all baselines aresampled throughout August 2023.We manually write 30 test questions that arerelated to financial markets and investment. Foreach question, we generate a single response fromInvestLM and the three commercial models. Wethen ask the financial experts to compare InvestLMresponses to each of the baselines and label whichresponse is better or whether neither response issignificantly better than the other.In addition to the expert evaluation, we also con-duct a GPT-4 evaluation, following the same pro-tocol used in (Zhou et al., 2023). Specifically, wesend GPT-4 with exactly the same instructions anddata annotations, and ask GPT-4 which response isbetter or whether neither response is significantlybetter than the other. The expert evaluation inter-face and GPT-4 evaluation prompt are presented inAppendix B.The expert evaluation and GPT-4 evaluationresults are presented in Figure 1 and Figure')],\n",
       " 'question': 'What is RAPTOR?'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with only filtered relevant documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "    for d in documents:\n",
    "        score = retrieval_grader.invoke(\n",
    "            {\"question\": question, \"document\": d.page_content}\n",
    "        )\n",
    "        grade = score.binary_score\n",
    "        if grade == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            continue\n",
    "    return {\"documents\": filtered_docs, \"question\": question}\n",
    "\n",
    "from langchain_core.documents.base import Document\n",
    "state = {\"question\": \"What is RAPTOR?\", 'documents':[Document(metadata={'description': 'Large language models (LLMs) like ChatGPT and GPT-4 are incredibly powerful, but they struggle to keep up with new information and understand long, complex documents. Traditional retrieval methods…', 'language': 'en', 'source': 'https://medium.com/@tuhinsharma121/raptor-a-smarter-way-to-retrieve-and-use-information-in-ai-fd3cb68a6f2f', 'title': 'RAPTOR: A Smarter Way to Retrieve and Use Information in AI | by Tuhin Sharma | Mar, 2025 | Medium'}, page_content='1 created summaries/clusters: 1Generating embeddings for level 2.Performing clustering for level 2.Generating summaries for level 2 with 1 clusters.Level 2 created summaries/clusters: 1Step 8: Query RAPTOR for AnswersThis retrieves the most relevant answer from the structured data. This step is heavily influenced by how we want to retrieve the documents from the vector store. There are basically two options.Credit : Parth Sarthi et alCollapsed tree collapses the tree into a single layer and retrieves nodes until a threshold number of tokens is reached, based on cosine similarity to the query vector. The nodes on which cosine similarity search is performed are highlighted in both illustrations.nodes = raptor_pack.run(    \"What baselines is InvestLM compared against?\", mode=\"collapsed\")print(len(nodes))print(nodes[0].text)2Baselines. We compare InvestLM with three state-of-the-art commercial models, GPT-3.5, GPT-4and Claude-2. OpenAI’s GPT-3.5 and GPT-4 arelarge language models tuned with reinforcementlearning from human feedback (RLHF) (Ouyanget al., 2022). Anthropic’s Claude-2 is a large lan-guage model that can take up to 100K tokens in theuser’s prompt. 3 Responses from all baselines aresampled throughout August 2023.We manually write 30 test questions that arerelated to financial markets and investment. Foreach question, we generate a single response fromInvestLM and the three commercial models. Wethen ask the financial experts to compare InvestLMresponses to each of the baselines and label whichresponse is better or whether neither response issignificantly better than the other.In addition to the expert evaluation, we also con-duct a GPT-4 evaluation, following the same pro-tocol used in (Zhou et al., 2023). Specifically, wesend GPT-4 with exactly the same instructions anddata annotations, and ask GPT-4 which response isbetter or whether neither response is significantlybetter than the other. The expert evaluation inter-face and GPT-4 evaluation prompt are presented inAppendix B.The expert evaluation and GPT-4 evaluationresults are presented in Figure 1 and Figure')]}\n",
    "grade_documents(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6400e8-c5af-4f91-a3e0-0c8b4fa511fc",
   "metadata": {},
   "source": [
    "# 4. [Generate Node] Write Generation logic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efbd7f6-2dd2-436f-9edf-9974ed9937f6",
   "metadata": {},
   "source": [
    "## 4.1. Import Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd21b6e7-5104-4a26-a10e-29cab984c7cb",
   "metadata": {},
   "source": [
    "- hub.pull: Fetches a pre-defined prompt template from LangChain Hub.\n",
    "- StrOutputParser: Used to convert the LLM response into a plain string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4302f85e-9cc3-412f-aca3-ad78a8c83934",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38cfb6f-bc16-4201-839c-b7ac891109bd",
   "metadata": {},
   "source": [
    "# 4.2. Define Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf3482d-5e1d-40c3-86df-bf95abf98d03",
   "metadata": {},
   "source": [
    "Pulls a community-defined RAG-style prompt template (likely expects context and question as input keys)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c5671af3-964e-4ae1-9255-84df32c02eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f323efa3-50a4-45aa-bfc0-35b92f82b43d",
   "metadata": {},
   "source": [
    "## 4.3. LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4020d32a-dd88-4735-bf5f-74465fbaefd9",
   "metadata": {},
   "source": [
    "Sets up an OpenAI chat model with deterministic output (temperature=0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e5a2c25e-9547-4d05-9fd7-df2d3d9fb157",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefd4aad-b655-4e30-9cb1-5c40502fa60f",
   "metadata": {},
   "source": [
    "## 4.4. Create RAG Chain:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052cb25d-f4df-4669-9c52-4534befb58e7",
   "metadata": {},
   "source": [
    "Composes a chain where:\n",
    " - The prompt is filled with context and question.\n",
    " - The LLM generates an answer.\n",
    " - The result is parsed to a clean string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b86c1a02-686f-429c-a3d6-2e935bf6ee28",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888afa13-5acc-4dee-94aa-7eb37a7ea9ce",
   "metadata": {},
   "source": [
    "## 4.5. Test Run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911a3769-0933-4fb7-bf25-1682456a813f",
   "metadata": {},
   "source": [
    "Runs the chain with a test question: \"agent memory\" and a docs variable (not defined in the snippet but assumed to be a list of retrieved documents with .page_content attributes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3db53bb4-fcc5-4ea4-baf2-05dea1b0924e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know.\n"
     ]
    }
   ],
   "source": [
    "question = \"agent memory\"\n",
    "generation = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b8fefd-2cf4-42b2-b758-3875c5d3713d",
   "metadata": {},
   "source": [
    "## 4.6. Create Generate Node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed0aa7d-cbab-4081-9d17-51f6bbef98b5",
   "metadata": {},
   "source": [
    "This function is a reusable wrapper for the RAG process. It:  \n",
    "Takes a state dictionary with:  \n",
    " - \"question\": the query string.  \n",
    " - \"documents\": the list of documents.  \n",
    "\n",
    "Passes them to the rag_chain.  \n",
    "Returns an updated state that includes:  \n",
    " - Original question  \n",
    " - Original documents  \n",
    " - Generated answer under the \"generation\" key.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f4090e47-f377-4d34-9fdd-d2c3b30dc1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # RAG generation\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": generation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e639abb6-2cb0-40cd-8081-0a7ee651ebaf",
   "metadata": {},
   "source": [
    "# 5. [Decision] - Hallucination and Answer Grader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb71e885-a85a-4fad-8bfe-1f4bb1b0fcbf",
   "metadata": {},
   "source": [
    "1. **Hallucination Detection** — Is the answer grounded in the provided documents?\n",
    "2. **Answer Relevance** — Does the answer address the user's question?\n",
    "\n",
    "We'll use OpenAI's `gpt-3.5-turbo` model and leverage **structured output** via Pydantic-based schemas to grade the responses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680b223c-36fc-4a56-a217-0e9b3f85b2ae",
   "metadata": {},
   "source": [
    "## 5.1. Define a Binary Scoring Schema for Hallucinations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc038ec6-cb4e-48c6-a42b-e7051d2a21e2",
   "metadata": {},
   "source": [
    "\n",
    "We use a Pydantic `BaseModel` to define the output format of our hallucination grader.\n",
    "\n",
    "- If the generation is **grounded in the facts**, we return `\"yes\"`.\n",
    "- If not, we return `\"no\"`.\n",
    "\n",
    "This schema helps the LLM return consistent, structured outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "65bc0e1b-183a-42f3-bff0-bd5c93f5f40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradeHallucinations(BaseModel):\n",
    "    \"\"\"Binary score for hallucination present in generation answer.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Answer is grounded in the facts, 'yes' or 'no'\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d059f5a8-846f-42d9-9123-e4551757221c",
   "metadata": {},
   "source": [
    "## 5.2. Setup the LLM Grader with Structured Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fc1a35-0979-4dfb-8738-a5c827b16e61",
   "metadata": {},
   "source": [
    "We instantiate a ChatOpenAI model and configure it to return structured responses conforming to the GradeHallucinations schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d74df810-a07d-4e46-b76a-7352adb215ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/adaptive-rag-dev/lib/python3.11/site-packages/langchain_openai/chat_models/base.py:1390: UserWarning: Cannot use method='json_schema' with model gpt-3.5-turbo-0125 since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "structured_llm_grader = llm.with_structured_output(GradeHallucinations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313274b4-1a3d-4f06-a950-6f73a47e875a",
   "metadata": {},
   "source": [
    "## 5.3. Create a Prompt Template for Hallucination Grading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0329f0d0-8dda-48c3-af84-d474d8e9da5b",
   "metadata": {},
   "source": [
    "This prompt asks the LLM to decide if the generated answer is supported by the set of retrieved documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "576fa557-6d6d-4895-bd8a-7bb9d23d21a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "system = \"\"\"You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \\n \n",
    "     Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts.\"\"\"\n",
    "hallucination_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Set of facts: \\n\\n {documents} \\n\\n LLM generation: {generation}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a533230-e55b-42f9-a3f2-e0d8a82f857d",
   "metadata": {},
   "source": [
    "## 5.4. Build the Hallucination Grader Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4a3d15-df89-48bc-a4d0-4cfbe2cde29d",
   "metadata": {},
   "source": [
    "We pipe the hallucination prompt to the structured LLM grader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "da8aad4d-ec14-497e-8dde-9c11869c7b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "hallucination_grader = hallucination_prompt | structured_llm_grader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97304a93-9e12-4dbd-b29a-4006c5fdef69",
   "metadata": {},
   "source": [
    "## 5.5. Define a Binary Scoring Schema for Answer Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dcac0d-4397-4048-b76b-877503fe2036",
   "metadata": {},
   "source": [
    "We now evaluate whether the generated answer addresses the user’s question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "31f47803-9937-4d73-81f8-98b7dd565d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradeAnswer(BaseModel):\n",
    "    \"\"\"Binary score to assess answer addresses question.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Answer addresses the question, 'yes' or 'no'\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33230972-80fe-4443-83a5-a44b4f2676f2",
   "metadata": {},
   "source": [
    "## 5.6. Create a Prompt and Grader for Answer Relevance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601e2c2c-1a4f-4a89-8c54-ec006f68f97f",
   "metadata": {},
   "source": [
    "Just like with hallucinations, we set up a structured LLM grader and prompt for determining if the answer is useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "51cd995f-6288-41ba-8e6a-1c95cab0d1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "system = \"\"\"You are a grader assessing whether an answer addresses / resolves a question \\n \n",
    "     Give a binary score 'yes' or 'no'. Yes' means that the answer resolves the question.\"\"\"\n",
    "answer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"User question: \\n\\n {question} \\n\\n LLM generation: {generation}\"),\n",
    "    ]\n",
    ")\n",
    "answer_grader = answer_prompt | structured_llm_grader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddcd8e7-8b0c-4391-a10e-acd59d90e657",
   "metadata": {},
   "source": [
    "## 5.7. Grade Answer for Both Groundedness and Relevance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a45eeee-ad77-427e-bd96-175c15610112",
   "metadata": {},
   "source": [
    "The following function performs dual grading:\n",
    " - First, it checks whether the answer is grounded in the documents (i.e., not hallucinated).\n",
    " - If yes, it then checks if the answer is useful and addresses the original question.\n",
    "\n",
    "Returns one of:\n",
    " - \"useful\": if answer is grounded and addresses the question.\n",
    " - \"not useful\": if grounded but irrelevant.\n",
    " - \"not supported\": if hallucinated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c716f8af-fea7-4ed6-968c-509aa831e82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_generation_v_documents_and_question(state):\n",
    "    \"\"\"\n",
    "    Determines whether the generation is grounded in the document and answers question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK HALLUCINATIONS---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "\n",
    "    score = hallucination_grader.invoke(\n",
    "        {\"documents\": documents, \"generation\": generation}\n",
    "    )\n",
    "    grade = score.binary_score\n",
    "\n",
    "    # Check hallucination\n",
    "    if grade == \"yes\":\n",
    "        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
    "        # Check question-answering\n",
    "        print(\"---GRADE GENERATION vs QUESTION---\")\n",
    "        score = answer_grader.invoke({\"question\": question, \"generation\": generation})\n",
    "        grade = score.binary_score\n",
    "        if grade == \"yes\":\n",
    "            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
    "            return \"useful\"\n",
    "        else:\n",
    "            print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n",
    "            return \"not useful\"\n",
    "    else:\n",
    "        pprint(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
    "        return \"not supported\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8547578d-0b69-45f3-8028-d3f23ef762b1",
   "metadata": {},
   "source": [
    "# 6. [Rewrite Question Node] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42feb80f-fc4a-40b0-8e65-f3cf1755b33c",
   "metadata": {},
   "source": [
    "## 6.1 Load the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f975988-e3a7-45dd-91f4-1c710c74080f",
   "metadata": {},
   "source": [
    "We initialize a Chat-based Large Language Model (gpt-3.5-turbo-0125) with zero temperature to ensure deterministic, repeatable outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cabee5ed-0d97-4e65-a8d4-76e3d0fa94f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03c445e-9939-44f2-91b6-8637d4e82726",
   "metadata": {},
   "source": [
    "## 6.2. Define the Prompt Template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b680f9d-8fd0-428f-9b93-00ef2ed7031a",
   "metadata": {},
   "source": [
    "- We define a system message guiding the LLM to act as a question re-writer.\n",
    "- The human message template takes in a {question} and instructs the model to improve it.\n",
    "- ChatPromptTemplate.from_messages is used to structure the multi-turn chat prompt for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "250f280c-b23a-46ec-b3f1-4744ebb488ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "system = \"\"\"You a question re-writer that converts an input question to a better version that is optimized \\n \n",
    "     for vectorstore retrieval. Look at the input and try to reason about the underlying semantic intent / meaning.\"\"\"\n",
    "re_write_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\",\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697e4bbf-b484-4301-a028-1c9e698762ba",
   "metadata": {},
   "source": [
    "## 6.3. Compose the Rewriting Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f1624f-15dd-4ea8-a658-2651a9452676",
   "metadata": {},
   "source": [
    "- This creates a LangChain pipeline: Prompt → LLM → Output Parser\n",
    "- The StrOutputParser() ensures we extract clean text from the LLM response.\n",
    "- We use invoke() to trigger the rewriting logic with the original question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f1623bc5-78f5-4c9f-aa61-3ca9d2842d02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"What is the role of memory in an agent's functioning?\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_rewriter = re_write_prompt | llm | StrOutputParser()\n",
    "question_rewriter.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d90af0-c32d-4c9b-927d-7f45de6bba16",
   "metadata": {},
   "source": [
    "## 6.4. Define a Transform Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced81305-56e6-428f-a477-1cd9e5ee4419",
   "metadata": {},
   "source": [
    "This function is useful in retrieval-augmented generation (RAG) pipelines:\n",
    "- Input: A dictionary state containing the original question and documents.\n",
    "- Action: Rewrites the question using the question_rewriter chain.\n",
    "- Output: Returns a new state with the same documents but an improved question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b5031331-2cb9-4bcc-8156-62730128e588",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_query(state):\n",
    "    \"\"\"\n",
    "    Transform the query to produce a better question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates question key with a re-phrased question\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Re-write question\n",
    "    better_question = question_rewriter.invoke({\"question\": question})\n",
    "    return {\"documents\": documents, \"question\": better_question}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441727a6-28b0-43fb-8f51-0fbdb68bf379",
   "metadata": {},
   "source": [
    "# 7. [Web Search Node]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85672bc0-c7dc-4f1a-bf0f-194aa8ccc758",
   "metadata": {},
   "source": [
    "## 7.1. Web Search with LangChain using TavilySearchResults\n",
    "\n",
    "we'll demonstrate how to use the `TavilySearchResults` tool from LangChain to perform real-time web searches. This is useful when building retrieval-augmented generation (RAG) applications that need fresh or dynamic content from the internet.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6c6dd2c6-fd9d-429b-83d9-36b4c6cde473",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "web_search_tool = TavilySearchResults(k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413d77f2-0358-4b98-9c8c-b19a397d3cf3",
   "metadata": {},
   "source": [
    "## 7.2. Define the Search Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d90541-d264-49fa-a50a-e0b039c98783",
   "metadata": {},
   "source": [
    "We'll define a `web_search` function that accepts a question and fetches top search results, which can later be used in downstream tasks like question answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "477418c9-e1df-4372-88d5-26b67cf98084",
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_search(state):\n",
    "    \"\"\"\n",
    "    Web search based on the re-phrased question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with appended web results\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---WEB SEARCH---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Web search\n",
    "    docs = web_search_tool.invoke({\"query\": question})\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "    web_results = Document(page_content=web_results)\n",
    "\n",
    "    return {\"documents\": web_results, \"question\": question}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b94c8ca-6f39-4762-acf3-5bca702aec7d",
   "metadata": {},
   "source": [
    "# 8. [Decision - Generate or Rewrite question]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edef1f53-9c13-4f10-b937-e1e42b143cc5",
   "metadata": {},
   "source": [
    "Determines whether to generate an answer, or re-generate a question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "10126e67-8c0b-4cf8-802e-2c0458ac0409",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or re-generate a question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    state[\"question\"]\n",
    "    filtered_documents = state[\"documents\"]\n",
    "\n",
    "    if not filtered_documents:\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\n",
    "            \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\"\n",
    "        )\n",
    "        return \"transform_query\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc67bcc-ed5c-4899-b487-e0e71488e17e",
   "metadata": {},
   "source": [
    "# 9. Construct the Agentic Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a998b22-140b-4393-992f-c7676bd4f9cd",
   "metadata": {},
   "source": [
    "## 9.1. Imports and Type Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b944e4f-6d1c-4bf6-8e81-3919a40018df",
   "metadata": {},
   "source": [
    "1. StateGraph is a class that lets you build a flow between different “states” or nodes.\n",
    "2. START and END are markers for the start and end of the workflow.\n",
    "3. GraphState is a TypedDict that defines the data passed between nodes:\n",
    " - question: input query\n",
    " - generation: LLM-generated answer\n",
    " - documents: list of retrieved or searched documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6c6225b7-287b-412b-9e83-4af51d012f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "from typing import List\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        documents: list of documents\n",
    "    \"\"\"\n",
    "\n",
    "    question: str\n",
    "    generation: str\n",
    "    documents: List[str]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e467f36d-fcee-42c0-b4e2-e9f608f06539",
   "metadata": {},
   "source": [
    "## 9.2. Create a StateGraph Instance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace86fcf-d735-4f4e-9bd6-721ecd06dde8",
   "metadata": {},
   "source": [
    "This initializes the graph with the GraphState schema — all nodes will share and update this state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "91261154-1543-4c6d-950b-665cbcea5e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(GraphState)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0261bde-f363-49e9-8186-b851de7760e1",
   "metadata": {},
   "source": [
    "## 9.3. Add Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dcad84-eec1-4e77-b11c-0c7eeddd63c6",
   "metadata": {},
   "source": [
    "Each node represents a function (assumed to be defined elsewhere) that operates on the GraphState. These nodes probably do the following:  \n",
    "\t•\tweb_search: Search the web for relevant content.  \n",
    "\t•\tretrieve: Pull documents from a vector store or database.  \n",
    "\t•\tgrade_documents: Score the quality/relevance of the retrieved docs.  \n",
    "\t•\tgenerate: Use LLM to generate an answer.  \n",
    "\t•\ttransform_query: Refine or rewrite the query if needed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e5138af0-059c-4a3b-a4c9-7b5d7da5e01b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x10e7b5a10>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.add_node(\"web_search\", web_search)  # web search\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
    "workflow.add_node(\"grade_documents\", grade_documents)  # grade documents\n",
    "workflow.add_node(\"generate\", generate)  # generatae\n",
    "workflow.add_node(\"transform_query\", transform_query)  # transform_query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ae7217-af36-42d3-ad86-7735ef71b0fa",
   "metadata": {},
   "source": [
    "## 9.4. Add Conditional Edges (Decision-Making)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a58b527-1a4c-4ba6-8441-46857e19ca19",
   "metadata": {},
   "source": [
    "route_question is a function (defined elsewhere) that decides where to route the question first — either to \"web_search\" or \"retrieve\" — based on some logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "78b1843b-08fb-45b7-8cf7-6f13e338da48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x10e7b5a10>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.add_conditional_edges(\n",
    "    START,\n",
    "    route_question,\n",
    "    {\n",
    "        \"web_search\": \"web_search\",\n",
    "        \"vectorstore\": \"retrieve\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"web_search\", \"generate\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"transform_query\": \"transform_query\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"transform_query\", \"retrieve\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    grade_generation_v_documents_and_question,\n",
    "    {\n",
    "        \"not supported\": \"generate\",\n",
    "        \"useful\": END,\n",
    "        \"not useful\": \"transform_query\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63beec8d-30c2-4ab6-acbc-27583e3ec25e",
   "metadata": {},
   "source": [
    "## 9.5. Compile the Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b7201e-d423-4a1c-8a39-8e17730c5c80",
   "metadata": {},
   "source": [
    "Final step to compile the graph into a runnable application (app) that you can now call with inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5cc4d2d2-14a1-42f6-9ee6-e4b191e09a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b16aba-454f-417f-a9e5-a95860157be6",
   "metadata": {},
   "source": [
    "# Example - The answer is not present in vectordb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dd130200-8ce4-42a7-88c8-6f63afc7a5e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---ROUTE QUESTION---\n",
      "---ROUTE QUESTION TO WEB SEARCH---\n",
      "---WEB SEARCH---\n",
      "\"Node 'web_search':\"\n",
      "'\\n---\\n'\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
      "---GRADE GENERATION vs QUESTION---\n",
      "---DECISION: GENERATION ADDRESSES QUESTION---\n",
      "\"Node 'generate':\"\n",
      "'\\n---\\n'\n",
      "'The share price of IBM is $251.33.'\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Run\n",
    "inputs = {\n",
    "    \"question\": \"what is the share price of IBM?\"\n",
    "}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        pprint(f\"Node '{key}':\")\n",
    "        # pprint(value, indent=2, width=80, depth=None)\n",
    "    pprint(\"\\n---\\n\")\n",
    "\n",
    "# Final generation\n",
    "pprint(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfc2a10-0b0e-47fa-83e2-d1a8d5fd83dd",
   "metadata": {},
   "source": [
    "# Example - The answer not present in vectordb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4a5a0724-a7fd-427f-90ca-0a8fe22b2c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---ROUTE QUESTION---\n",
      "---ROUTE QUESTION TO RAG---\n",
      "---RETRIEVE---\n",
      "\"Node 'retrieve':\"\n",
      "'\\n---\\n'\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE---\n",
      "\"Node 'grade_documents':\"\n",
      "'\\n---\\n'\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
      "---GRADE GENERATION vs QUESTION---\n",
      "---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\n",
      "\"Node 'generate':\"\n",
      "'\\n---\\n'\n",
      "---TRANSFORM QUERY---\n",
      "\"Node 'transform_query':\"\n",
      "'\\n---\\n'\n",
      "---RETRIEVE---\n",
      "\"Node 'retrieve':\"\n",
      "'\\n---\\n'\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE---\n",
      "\"Node 'grade_documents':\"\n",
      "'\\n---\\n'\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
      "---GRADE GENERATION vs QUESTION---\n",
      "---DECISION: GENERATION ADDRESSES QUESTION---\n",
      "\"Node 'generate':\"\n",
      "'\\n---\\n'\n",
      "('RAPTOR stands for Recursive Abstractive Processing for Tree-Organized '\n",
      " 'Retrieval. It is a new retrieval system that organizes information in a tree '\n",
      " 'structure to efficiently retrieve and summarize text. RAPTOR works by '\n",
      " 'clustering and summarizing text at multiple levels to provide both '\n",
      " 'fine-grained details and high-level summaries.')\n"
     ]
    }
   ],
   "source": [
    "# Run\n",
    "inputs = {\"question\": \"what is the full form of RAPTOR?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        pprint(f\"Node '{key}':\")\n",
    "        # pprint(value, indent=2, width=80, depth=None)\n",
    "    pprint(\"\\n---\\n\")\n",
    "\n",
    "# Final generation\n",
    "pprint(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7076965-be87-422b-a21d-757496ffbfe5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
